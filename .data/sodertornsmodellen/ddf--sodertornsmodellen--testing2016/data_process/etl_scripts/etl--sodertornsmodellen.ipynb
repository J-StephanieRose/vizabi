{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL script for sodertornsmodellen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates ddf--list and ddf--data_for files for Södertornsmodellen and converts shapefiles to GeoJSON.\n",
    "\n",
    "ARGUMENTS: /path/to/ddf /path/to/raw\n",
    "\n",
    "SYNOPSIS: python etl--sodertornsmodellen.py /path/to/ddf /path/to/raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import shapefile\n",
    "from json import dumps\n",
    "reload(sys)\n",
    "sys.setdefaultencoding(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_ddf_list(df, cols, entity, target_dir, index_file=None, prefix='ddf--list--',value_concepts=None):\n",
    "    \"\"\"\n",
    "    ARGUMENTS\n",
    "    \"\"\"  \n",
    "    outfile = target_dir + prefix + entity + '.csv'\n",
    "    df[cols].to_csv(outfile, encoding='UTF-8',index=False)\n",
    "    print 'Printed ' + outfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_ddf_datafor(df, cols, target_dir, dimensions, measure=None, prefix='ddf--data_for--', index_file=None, aliases=None):\n",
    "    \"\"\"\n",
    "    ARGUMENTS\n",
    "    \"\"\"\n",
    "    #assumes measure is given as last element in cols\n",
    "    if measure is None:\n",
    "        measure = cols[-1]\n",
    "    outfile = target_dir + prefix + measure + '--by--' + '--'.join(dimensions) + '.csv'\n",
    "        \n",
    "    if aliases is not None:\n",
    "        df[cols].to_csv(outfile, encoding='UTF-8',index=False, header=aliases)\n",
    "    else:\n",
    "        df[cols].to_csv(outfile, encoding='UTF-8',index=False)\n",
    "    print 'Printed ' + outfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shp2geojson(shp_file, json_file=None):\n",
    "    \n",
    "    #read shapefile\n",
    "    reader = shapefile.Reader(shp_file)\n",
    "    fields = reader.fields[1:]\n",
    "    field_names = [field[0] for field in fields]\n",
    "    features = []\n",
    "    for sr in reader.shapeRecords():\n",
    "        atr = dict(zip(field_names, sr.record))\n",
    "        geom = sr.shape.__geo_interface__\n",
    "        features.append(dict(type='Feature', geometry=geom, properties=atr))\n",
    "\n",
    "    if json_file is not None:\n",
    "        #write GeoJSON file\n",
    "        geojson = open(json_file, 'w')\n",
    "        geojson.write(dumps({'type': 'FeatureCollection', 'features': features}, indent=2) + \"\\n\")\n",
    "        geojson.close()\n",
    "        print 'Printed ' + json_file\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def excel2csv(file_xlx, file_csv):\n",
    "    df = pd.read_excel(file_xlx)\n",
    "    df.to_csv(file_csv, index=False)\n",
    "    print 'Printed ' + file_csv\n",
    "    del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lookup tables used in correcting/translating column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#replacements using regex\n",
    "rep = {unicode('å'): 'a',\n",
    "       unicode('ä'): 'a',\n",
    "       unicode('ö'): 'o',\n",
    "       unicode('år'): 'year',\n",
    "       '  ': '',\n",
    "       ' ': '_',\n",
    "       'plomr namn': 'planomradesnamn'\n",
    "      }\n",
    "rep = dict((re.escape(k), v) for k, v in rep.iteritems())\n",
    "pattern = re.compile(\"|\".join(rep.keys()))\n",
    "\n",
    "#example: the call below will return 'text' with replacements according to 'rep'\n",
    "#pattern.sub(lambda m: rep[re.escape(m.group(0))], text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#take folder separator as per os\n",
    "pathspt = os.path.sep\n",
    "\n",
    "#define root path\n",
    "root = '..' + pathspt + '..' + pathspt + '..' + pathspt + 'ddf--sodertornsmodellen--testing2016'\n",
    "#root=str(sys.argv[1])\n",
    "\n",
    "#path to raw data files (cannot get it directly from github since repo is private)\n",
    "raw = '..' + pathspt + '..' + pathspt + '..' + pathspt + 'raw--sodertornsmodellen--testing2016'\n",
    "#raw = str(sys.argv[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basomrade_file = raw + pathspt + 'Kopia av Basområden utan småområden 151103.xlsx'\n",
    "kommun_file = raw + pathspt + 'Kopia av kommuner mindre fil 151022.xlsx'\n",
    "df_kommun = pd.read_excel(kommun_file,'Blad1')\n",
    "df_basomrade = pd.read_excel(basomrade_file,'Blad1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing (remove unnecessary columns and modify column names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del df_kommun['Kommun.1']\n",
    "df_kommun.columns = [pattern.sub(lambda m: rep[re.escape(m.group(0))], col.lower()) for col in df_kommun.columns]\n",
    "df_basomrade.columns = [pattern.sub(lambda m: rep[re.escape(m.group(0))], col.lower()) for col in df_basomrade.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ddf--list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geo_dims = ['kommun','basomrade']\n",
    "time_dims = ['year']\n",
    "list_cols_kommun = 'kommun'\n",
    "list_cols_basomrade = ['basomradeskod', 'basomrade','kommun']\n",
    "list_header_kommun = ['geo', 'name', 'is.kommun']\n",
    "list_header_basomrade = ['geo','name','kommun','is.basomrade']\n",
    "\n",
    "#drop duplicates\n",
    "s_list_kommun_name = df_kommun[list_cols_kommun].drop_duplicates()\n",
    "df_list_basomrade = df_basomrade[list_cols_basomrade].drop_duplicates(subset = 'basomrade')\n",
    "\n",
    "#create geo id for kommun by making lowercase and removing åäö and blank space\n",
    "geo_id = [pattern.sub(lambda m: rep[re.escape(m.group(0))], kommun.lower()) for kommun in s_list_kommun_name]\n",
    "s_list_kommun_geo = pd.Series(geo_id, name = 'geo')\n",
    "\n",
    "#concatenate series\n",
    "df_list_kommun = pd.concat([s_list_kommun_geo, s_list_kommun_name], axis = 1)\n",
    "del s_list_kommun_name\n",
    "del s_list_kommun_geo\n",
    "\n",
    "#create a lookup table that converts name (kommun) to geo (id)\n",
    "kommun2id = {row.kommun : row.geo for i,row in df_list_kommun.iterrows()}\n",
    "\n",
    "#add is.kommun and is.basomrade\n",
    "df_list_kommun['is.kommun'] = True\n",
    "df_list_basomrade['is.basomrade'] = True\n",
    "\n",
    "#change headers\n",
    "df_list_kommun.columns = list_header_kommun\n",
    "df_list_basomrade.columns = list_header_basomrade\n",
    "\n",
    "#populate column 'kommun' with geo id instead of name\n",
    "df_list_basomrade['kommun'] = [kommun2id[unicode(kommun)] for kommun in df_list_basomrade['kommun']]\n",
    "\n",
    "#add lonlat for kommuner\n",
    "df_lonlat = pd.read_csv(root + pathspt + 'data_process' + pathspt + 'coords_kommuner.csv')\n",
    "df_list_kommun['latitude'] = df_lonlat['geo.latitude']\n",
    "df_list_kommun['longitude'] = df_lonlat['geo.longitude']\n",
    "\n",
    "#write csv files\n",
    "frames = [df_list_kommun, df_list_basomrade]\n",
    "for i,df in enumerate(frames):\n",
    "    write_ddf_list(df, df.columns, geo_dims[i], root + pathspt, prefix='ddf--list--geo--')\n",
    "\n",
    "#free space\n",
    "del df_lonlat\n",
    "del df_list_kommun\n",
    "del df_list_basomrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ddf--data_for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "measures = ['medelinkomst', 'andel_hogskoleutbildade', 'befolkning', 'forvarvsfrekvens', 'flytt_arbete', 'flytt_utbildning']\n",
    "datafor_kommun_fixedcols = ['geo', 'kommun', 'year']\n",
    "datafor_basomrade_fixedcols = ['basomradeskod', 'basomrade', 'year']\n",
    "fixedcols = [datafor_kommun_fixedcols, datafor_basomrade_fixedcols]\n",
    "kommun_dims = ['kommun', 'year']\n",
    "basomrade_dims = ['basomrade', 'year']\n",
    "dims = [kommun_dims, basomrade_dims]\n",
    "\n",
    "#sort tables by kommun/basomrade and year\n",
    "df_kommun_sorted = df_kommun.sort_values(kommun_dims)\n",
    "df_basomrade_sorted = df_basomrade.sort_values(basomrade_dims)\n",
    "\n",
    "#insert a geo (id) column\n",
    "df_kommun_sorted.insert(0, 'geo', [kommun2id[unicode(kommun)] for kommun in df_kommun_sorted['kommun']])\n",
    "\n",
    "#write csv files\n",
    "frames = [df_kommun_sorted, df_basomrade_sorted]\n",
    "for i, df in enumerate(frames):\n",
    "    for measure in measures:\n",
    "        write_ddf_datafor(df, fixedcols[i] + [measure], root + pathspt, dims[i], aliases=['geo', 'name', 'year', measure])    \n",
    "\n",
    "#free space\n",
    "del df_kommun\n",
    "del df_basomrade\n",
    "del df_kommun_sorted\n",
    "del df_basomrade_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ddf--measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create xlsx for ddf--measures (this can later be edited manually)\n",
    "measuresfile_xlx = root + pathspt + 'data_process' + pathspt + 'ddf--measures.xlsx'\n",
    "measuresfile_csv = root + pathspt + 'ddf--measures.csv'\n",
    "\n",
    "if not os.path.isfile(measuresfile_xlx):\n",
    "    measures_header = ['measure', 'name', 'name_short', 'description']\n",
    "    df_measures = pd.DataFrame(columns = measures_header)\n",
    "    df_measures['measure'] = measures\n",
    "    #output as excel sheet\n",
    "    df_measures.to_excel(measuresfile_xlx, index=False)\n",
    "    del df_measures\n",
    "    print 'Printed' + measuresfile_xlx\n",
    "    excel2csv(measuresfile_xlx, measuresfile_csv)\n",
    "else:\n",
    "    print 'File: ' + measuresfile_xlx + ' already exists.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ddf--dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use ddf--dimension from systema globalis as template\n",
    "dimfile_xlx = root + pathspt + 'data_process' + pathspt + 'ddf--dimensions.xlsx'\n",
    "dimfile_csv = root + pathspt + 'ddf--dimensions.csv'\n",
    "csv_url = 'https://raw.githubusercontent.com/valor-software/ddf--gapminder--systema_globalis/master/ddf--dimensions.csv'\n",
    "\n",
    "if not os.path.isfile(dimfile_xlx):\n",
    "    df_dim = pd.read_csv(csv_url)\n",
    "    #delete last column (it's empty)\n",
    "    del df_dim[df_dim.columns[-1]]\n",
    "    #find rows with type = dimension or name = Year and drop the rest\n",
    "    df_dim_reduced = df_dim[(df_dim['type'] == 'dimension') | (df_dim['name'] == 'Year')]\n",
    "    #add map column\n",
    "    df_dim_reduced['map'] = ''\n",
    "    df_dim_reduced.to_excel(dimfile_xlx, index=False)\n",
    "    del df_dim\n",
    "    print 'Printed ' + dimfile_xlx\n",
    "    excel2csv(dimfile_xlx, dimfile_csv)\n",
    "else:\n",
    "    print 'File: ' + dimfile_xlx + ' already exists.'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract zips, convert shapefiles to geojson and write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extract_dir = root + pathspt + 'extracted'\n",
    "json_dir = root + pathspt + 'geojson'\n",
    "input_names = ['Bas_Sodertorn_2000', 'Bas_Sodertorn_2010']\n",
    "zip_files = [raw + pathspt + name + '.zip' for name in input_names] \n",
    "\n",
    "if not os.path.isdir(extract_dir):\n",
    "    os.makedirs(extract_dir)\n",
    "    \n",
    "if not os.path.isdir(json_dir):\n",
    "    os.makedirs(json_dir)\n",
    "    \n",
    "for idx, zfile in enumerate(zip_files):\n",
    "    fh = open(zfile, 'rb')\n",
    "    z = zipfile.ZipFile(fh)\n",
    "\n",
    "    #extract files\n",
    "    for name in z.namelist():\n",
    "        z.extract(name, extract_dir)\n",
    "        if name[-4:] == '.shp':\n",
    "            shp_file = name\n",
    "            \n",
    "    #convert shapefile to geojson\n",
    "    shp2geojson(extract_dir + pathspt + shp_file, json_dir + pathspt + input_names[idx].lower() + '.json')\n",
    "    \n",
    "    fh.close()\n",
    "    z.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ddf--list--geo--basomrade--map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shpfile = root + pathspt + 'extracted' + pathspt + 'Bas_Sodertorn_2010' + pathspt + 'Basomr_Sod_2010.shp'\n",
    "features = shp2geojson(shpfile)\n",
    "listfile = root + pathspt + 'ddf--list--geo--basomrade.csv'\n",
    "mapfile = root + pathspt + 'ddf--list--geo--basomrade--map.csv'\n",
    "shpfile_identifier = 'BASKOD2010'\n",
    "\n",
    "#load ddf--list--geo--basomrade and add column map\n",
    "df_list = pd.read_csv(listfile)\n",
    "df_geoshape = df_list['geo'].to_frame()\n",
    "df_geoshape['map'] = ''\n",
    "\n",
    "#add geojson feature to each basomrade\n",
    "for feature in features:\n",
    "    baskod = feature['properties'][shpfile_identifier]\n",
    "    rowindex = df_geoshape.loc[df_list['geo'] == baskod].index\n",
    "    df_geoshape.loc[rowindex, 'map'] = str(feature)\n",
    "\n",
    "#write to csv\n",
    "df_geoshape.to_csv(mapfile, index=False)\n",
    "print 'Printed ' + mapfile\n",
    "\n",
    "#free space\n",
    "del df_list\n",
    "del df_geoshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ddf--index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index_header = ['file', 'value_concept', 'geo', 'time']\n",
    "outfile = root + pathspt + 'ddf--index.csv'\n",
    "\n",
    "#specify value_concepts of list files (cannot read this based on file name only as we do for data_for)\n",
    "#would also be possible to read in csv in order to get the same info but that way is much slower\n",
    "lookup_value_concept = {'kommun': ['name', 'is.kommun','latitude','longitude'], 'basomrade': ['name','kommun','is.basomrade'], 'map': ['map']}\n",
    "\n",
    "#create ddf--index file\n",
    "pd.DataFrame(columns = index_header).to_csv(outfile, encoding='UTF-8', index=False)\n",
    "indexfile = open(outfile, 'a')\n",
    "\n",
    "for item in os.listdir(root + pathspt):\n",
    "    #add enties for data_for files\n",
    "    if re.search('--data_for--', item):\n",
    "        fragments = item.split('.')[0].split('--')\n",
    "        time = fragments[-1]\n",
    "        geo = fragments[-2]\n",
    "        value_concept = fragments[-4]\n",
    "        indexfile.write(','.join([item, value_concept, geo, time]) + '\\n')\n",
    "    \n",
    "    #add entries for ddf--list files (for now this assumes that the only list--files that exist are list--geo)   \n",
    "    elif re.search('--list--geo', item):\n",
    "        fragments = item.split('.')[0].split('--')\n",
    "        if fragments[-1] == 'map':\n",
    "            geo = fragments[-2] \n",
    "        else:\n",
    "            geo = fragments[-1]\n",
    "        time = ''\n",
    "        value_concepts = lookup_value_concept[fragments[-1]]\n",
    "        for vc in value_concepts:\n",
    "            indexfile.write(','.join([item, vc, geo, time]) + '\\n')\n",
    "            \n",
    "print 'Printed ddf--index.csv' \n",
    "indexfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
